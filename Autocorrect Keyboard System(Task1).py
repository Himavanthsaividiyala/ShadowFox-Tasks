# -*- coding: utf-8 -*-
"""Untitled44.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_IphBvJO6lHVt-CfgwVn-pIelnHVGVfR
"""

!pip install pyspellchecker tensorflow google-generativeai

!pip install python-dotenv

!pip install pyspellchecker





import google.generativeai as genai
from spellchecker import SpellChecker
import re
import numpy as np
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense

# Step 1: Set up Google Gemini API
API_KEY = "YOUR_GOOGLE_API_KEY"  # Replace with your actual API key
genai.configure(api_key=API_KEY)
gemini_model = genai.GenerativeModel("gemini-pro")

# Step 2: Load Corpus from File
def load_corpus_from_file(file_path):
    with open(file_path, 'r', encoding='utf-8') as f:
        return f.read()

# Step 3: Autocorrect Function (pyspellchecker)
def correct_sentence(sentence):
    spell = SpellChecker()
    words = sentence.split()
    corrected = []
    for word in words:
        if word in spell:
            corrected.append(word)
        else:
            corrected.append(spell.correction(word) or word)
    return ' '.join(corrected)

# Step 4: Clean and tokenize text
def clean_text(text):
    text = text.lower()
    text = re.sub(r'[^\w\s]', '', text)
    return text

# Step 5: Prepare LSTM Model

seq_len = 5  # Increased sequence length
tokenizer = None
lstm_model = None
total_words = 0

def prepare_lstm_model(corpus_text, epochs=30):
    global tokenizer, lstm_model, total_words

    corpus = clean_text(corpus_text)
    tokenizer = Tokenizer()
    tokenizer.fit_on_texts([corpus])
    total_words = len(tokenizer.word_index) + 1

    input_sequences = []
    tokens = tokenizer.texts_to_sequences([corpus])[0]
    for i in range(seq_len, len(tokens)):
        ngram_seq = tokens[i - seq_len:i + 1]
        input_sequences.append(ngram_seq)

    input_sequences = np.array(input_sequences)
    X, y = input_sequences[:, :-1], input_sequences[:, -1]
    y = to_categorical(y, num_classes=total_words)

    lstm_model = Sequential()
    lstm_model.add(Embedding(input_dim=total_words, output_dim=128))
    lstm_model.add(LSTM(256, return_sequences=True))
    lstm_model.add(LSTM(128))
    lstm_model.add(Dense(total_words, activation='softmax'))

    lstm_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
    lstm_model.fit(X, y, epochs=epochs, verbose=1)

# Step 6: Predict Top 3 Words with LSTM using temperature

def sample_top_n(preds, top_n=3):
    top_indices = np.argsort(preds)[-top_n:][::-1]
    return top_indices

def predict_next_words_lstm(seed_text, top_n=3):
    global lstm_model, tokenizer
    seed_text = clean_text(seed_text)
    token_list = tokenizer.texts_to_sequences([seed_text])[0]
    token_list = token_list[-seq_len:]
    padded = pad_sequences([token_list], maxlen=seq_len)
    prediction = lstm_model.predict(padded, verbose=0)[0]
    top_indices = sample_top_n(prediction, top_n)

    word_predictions = []
    for idx in top_indices:
        for word, index in tokenizer.word_index.items():
            if index == idx:
                word_predictions.append(word)
                break
    return word_predictions

# Step 7: Gemini Fallback
def predict_next_word_gemini(context):
    prompt = f"Predict the next word after this phrase: '{context.strip()}'"
    response = gemini_model.generate_content(prompt)
    predicted_text = response.text.strip()
    return predicted_text.split()[0] if predicted_text else ""

# Step 8: Hybrid Prediction
def hybrid_predict_next_words(user_input):
    corrected = correct_sentence(user_input)
    lstm_predictions = predict_next_words_lstm(corrected)

    if not lstm_predictions:
        ai_prediction = predict_next_word_gemini(corrected)
        return corrected, [ai_prediction]
    else:
        return corrected, lstm_predictions

# Step 9: Main loop
print("\n Hybrid Autocorrect + AI+LSTM Next-Word Prediction System")
print("Type 'exit' to quit\n")

# Step 10: Upload corpus before this step
corpus_text = load_corpus_from_file("/content/big_corpus.txt")  # Change this filename
prepare_lstm_model(corpus_text, epochs=30)

while True:
    user_input = input("You: ")
    if user_input.lower().strip() == 'exit':
        print("Exiting...")
        break

    corrected, suggestions = hybrid_predict_next_words(user_input)
    print(f"\n Corrected: {corrected}")
    print(f" Suggested Next Words: {' / '.join(suggestions)}\n")

